{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"[Actividad extracurricular 12] web scraping\"\n",
    "author: \"David Pilataxi\"\n",
    "date: 08/01/2025\n",
    "lang: es\n",
    "format: \n",
    "  pdf:\n",
    "    toc: true\n",
    "    toc-title: \"Tabla de Contenidos\"  \n",
    "execute:\n",
    "  echo: true\n",
    "  error: false\n",
    "  warning: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "- David Pilataxi\n",
    "- Gr1cc\n",
    "- 8 de enero de 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlace al Repositorio\n",
    "\n",
    "LINK: https://github.com/DavidPilataxi/MetodosNumericosGr1cc/blob/main/Actividad%20Extra%2012%20Web%20Scrapping/WebScraping.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objetivos\n",
    "- Revisar qué es web scraping\n",
    "- Realizar una prueba en python para dos librerías diferentes\n",
    "- Realizar scraping de un sitio web de su elección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introducción\n",
    "Es el proceso de extraer datos de sitios web de manera automatizada utilizando herramientas o scripts. Esto se logra accediendo al contenido de las páginas web, ya sea analizando el HTML, extrayendo datos estructurados, o interactuando con elementos dinámicos.\n",
    "\n",
    "Aplicaciones comunes de web scraping:\n",
    "\n",
    "- Análisis de precios en sitios de comercio electrónico.\n",
    "- Monitorización de noticias o contenidos en tiempo real.\n",
    "- Recolección de información para proyectos de investigación o estudios de mercado.\n",
    "- Extracción de datos de directorios en línea o bases de datos accesibles públicamente.\n",
    "\n",
    "Herramientas populares para web scraping:\n",
    "\n",
    "- BeautifulSoup (Python) para extraer datos de HTML.\n",
    "- Selenium para interactuar con páginas dinámicas.\n",
    "- Scrapy, un framework avanzado para tareas de scraping masivo.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Procedimiento\n",
    "Web Scraping: BeautifulSoup vs Scrapy\n",
    "\n",
    "**Configuración inicial**\n",
    "\n",
    "- !pip install requests beautifulsoup4 pandas scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Web Scraping con BeautifulSoup. -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos extraídos de Python.org:\n",
      "                                              Título  \\\n",
      "0     PSF Grants: Program & Charter Updates (Part 1)   \n",
      "1       PSF Grants: Program & Charter Updates (TLDR)   \n",
      "2     PSF Grants: Program & Charter Updates (Part 3)   \n",
      "3  Announcing Python Software Foundation Fellow M...   \n",
      "4                       Python 3.14.0 alpha 3 is out   \n",
      "\n",
      "                                              Enlace  \n",
      "0  https://pyfound.blogspot.com/2024/12/psf-grant...  \n",
      "1  https://pyfound.blogspot.com/2024/12/psf-grant...  \n",
      "2  https://pyfound.blogspot.com/2024/12/12psf-gra...  \n",
      "3  https://pyfound.blogspot.com/2024/12/announcin...  \n",
      "4  https://pythoninsider.blogspot.com/2024/12/pyt...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL del sitio web\n",
    "url = \"https://www.python.org/blogs/\"\n",
    "\n",
    "# Enviar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Ajustar los selectores para títulos y enlaces\n",
    "    titles = [title.text.strip() for title in soup.select(\"li h3\")]\n",
    "    links = [link['href'] for link in soup.select(\"li h3 a\")]\n",
    "\n",
    "    # Crear DataFrame\n",
    "    data = pd.DataFrame({\"Título\": titles, \"Enlace\": links})\n",
    "    data.to_csv(\"python_org_blogs.csv\", index=False)\n",
    "\n",
    "    print(\"Datos extraídos de Python.org:\")\n",
    "    print(data.head())\n",
    "else:\n",
    "    print(f\"Error: No se pudo acceder al sitio web. Código de estado {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Imagen1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Web Scraping con Scrapy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos en terminal:\n",
    "- pip install scrapy\n",
    "- scrapy startproject quotes_scraper\n",
    "- scrapy crawl quotes -o quotes.json  \n",
    "El siguiente es el código para el web scrapping, y acontinuación, se presenta el resultado tras correrlo desde la terminal en un archivo independiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQuotesSpider\u001b[39;00m(scrapy\u001b[38;5;241m.\u001b[39mSpider):\n\u001b[0;32m      2\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquotes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m     start_urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://quotes.toscrape.com/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer todas las citas de la página\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        # Seguir al siguiente página, si existe\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Imagen2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusiones\n",
    "- BeautifulSoup destaca por su simplicidad, lo que lo convierte en una opción ideal para proyectos pequeños o tareas de scraping sencillas.\n",
    "\n",
    "- Scrapy, por otro lado, es una herramienta escalable, diseñada para manejar proyectos más complejos o grandes volúmenes de datos, lo que lo convierte en la opción preferida para scraping masivo.\n",
    "\n",
    "- Entre las limitaciones comunes en el scraping, se encuentran los problemas legales, ya que es importante respetar las políticas de uso de los sitios web. Además, pueden surgir bloqueos o restricciones en los sitios que dificultan la recolección de datos.\n",
    "\n",
    "- Una consideración adicional es la gestión de la eficiencia, ya que Scrapy, al ser más robusto, permite manejar peticiones concurrentes y tiempos de espera, mientras que BeautifulSoup puede volverse menos eficiente en proyectos más grandes debido a su naturaleza más simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Referencias Bibliográficas\n",
    "- **Requests:**\n",
    "\n",
    "Reitz, K. (2023). Requests: HTTP for Humans. Python Software Foundation. Recuperado de https://docs.python-requests.org/en/latest/\n",
    "\n",
    "- **BeautifulSoup:**\n",
    "\n",
    "Richardson, L. (2023). Beautiful Soup Documentation. Python Software Foundation. Recuperado de https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "- **Pandas:**\n",
    "\n",
    "McKinney, W. (2010). Data Structures for Statistical Computing in Python. In Proceedings of the 9th Python in Science Conference (pp. 51-56). Recuperado de https://pandas.pydata.org/\n",
    "\n",
    "- **Scrapy:**\n",
    "\n",
    "Scrapy Developers. (2023). Scrapy Documentation. Recuperado de https://docs.scrapy.org/en/latest/\n",
    "\n",
    "- **Sitio web objetivo:**\n",
    "\n",
    "Quotes to Scrape. (s.f.). Recuperado de https://quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
